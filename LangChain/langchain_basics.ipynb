{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dd7ca9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c82628",
   "metadata": {},
   "source": [
    "\n",
    "* *Prompt Templates*: Um prompt basicamente é uma entrada de texto que fornecemos ao LLM que, por sua vez, processa e nos retona uma resposta. Contudo, quando estamos lidando com LLMs de forma programática, talvez queiramos usar um prompt dinâmico que recebe diferentes parâmetros. Essa é a primeira abstração que nos deparamos ao lidar com LangChain. Para isso temos o *PromptTemplate* que basicamente se trata de uma classe que envolve o prompt e nos permite exeutar nosso prompt várias vezes com difererntes entradas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d58a222e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bfac25",
   "metadata": {},
   "source": [
    "* *Chat models*: O objeto chat model funciona como a nossa interface primária para integarir com LLMs. Essa é a maneira padrão que o LangChain nos ajuda a conversar com os LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c176a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, dotenv_values\n",
    "\n",
    "# Carregando as variáveis do .env\n",
    "load_dotenv()\n",
    "# LangChain puxa automaticamente a variável de ambiente \"OPENAI_API_KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3331c104",
   "metadata": {},
   "source": [
    "No execício de SDK standard fizemos assim, lembra?\n",
    "\n",
    "```py\n",
    "config = dotenv_values(\"../.env\")\n",
    "openai_api_key = config[\"OPENAI_API_KEY\"]\n",
    "```\n",
    "Aqui estamos apenas carregando a chave e armazenando-a na variável \"openai_api_key\" mas o LangChain NÃO usa essa variável de maneira direta. Caso quiséssemo utilizá-la teríamos que fazer:\n",
    "```py\n",
    "llm = ChatOpenAI(temperature=1.0, model=\"gpt-4.1-mini\", api_key=openai_api_key)\n",
    "```\n",
    "### Vamos trabalhar com uma chamada simples. Primeiro passamos um contexto e, segundo esse contexto, vamos dar um comando e esperar uma resposta do LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aafb865d",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "    Cristiano Ronaldo dos Santos Aveiro é um futebolista português que atua como atacante, mais \n",
    "    especificamente como ponta-esquerda ou centroavante, no Al-Nassr e na Seleção Portuguesa, onde é capitão. \n",
    "    É o maior goleador da história do futebol em jogos oficiais, sendo simultaneamente o jogador \n",
    "    com mais gols na história a nível de seleções bem como a nível de clubes, trajetória que o levou ao recorde \n",
    "    de cinco nomeações ao prémio de melhor artilheiro do mundo pela Federação Internacional de\n",
    "    História e Estatísticas do Futebol (IFFHS). Durante sua passagem pela Europa, se tornou o maior jogador\n",
    "    histórico e recordista da UEFA Champions League, Nations League e Eurocopa. Também é o único atleta\n",
    "    a vencer nas três ligas europeias de maior prestígio (Premier League, La Liga e Série A) sendo eleito destaque\n",
    "    como o melhor jogador e artilheiro das mesmas, quando protagonizava a conquista de seus títulos.\n",
    "    Cristiano Ronaldo é frequentemente considerado o melhor e mais completo futebolista, bem como o maior\n",
    "    artilheiro do mundo e na opinião da grande maioria dos especialistas do esporte, seus atributos físicos, \n",
    "    suas habilidades goleadoras, sua mentalidade vencedora, sua liderança e seu desempenho sob pressão, \n",
    "    o tornam um dos melhores e maiores futebolistas de todos os tempos, com alguns ainda o colocando como o \n",
    "    melhor jogador de sempre. Futebolista histórico, é considerado o jogador do século pela Globe Soccer Awards por \n",
    "    diversos ex-jogadores, técnicos e dirigentes do futebol. Escalado no Dream Team da Bola de Ouro, \n",
    "    também é o jogador com o maior número de nomeações do prémio Ballon d'Or. Foi eleito o melhor jogador \n",
    "    do mundo pela FIFA e pela France Football recebendo o prémio Bola de Ouro (que por um período passou a \n",
    "    denominar-se Bola de Ouro da FIFA) um total de cinco vezes, sendo o segundo maior vencedor, atrás apenas de \n",
    "    Lionel Messi. Também venceu o prémio Bota de Ouro da UEFA num total de quatro vezes. Ganhou uma vez o prémio de \n",
    "    Melhor Jogador de Clubes da UEFA. Detém o recorde de três vezes o prémio de Melhor Jogador da UEFA na Europa, \n",
    "    três vezes FIFA The Best, seis vezes Globbe Soccer Awards e também o recorde de 15 nomeações na Equipe do Ano da UEFA. \n",
    "    Além disso detém quase todos os recordes da Champions League, como a artilharia geral, bem como é amplamente \n",
    "    considerado o maior jogador de toda história desta competição.\n",
    "    \"\"\"\n",
    "\n",
    "# Esse placeholder {context} faz referência a variável \"context\" definida logo acima \n",
    "prompt = \"\"\"    \n",
    "    Dado o contexto {context} sobre uma pessoa, eu quero que você crie:\n",
    "    1 - Um pequeno resumo dessa pessoa\n",
    "    2 - Dois fatos interessantes sobre essa pessoa\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c00def1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_tamplate = PromptTemplate(\n",
    "    input_variables=[\"context\"],    # Esse [\"context\"] deve corresponder ao placeholder de prompt\n",
    "    template=prompt\n",
    "    )\n",
    "\n",
    "# Essa linha é o que fará a comunicação com o gpt-4.1 mini e fará as chamadas via API \n",
    "llm = ChatOpenAI(temperature=1.0, model=\"gpt-4.1-mini\")\n",
    "\n",
    "# Esse linha faz o análogo mas utilizando o ollama\n",
    "edge_llm = ChatOllama(temperature=1.0, model=\"gemma3:270m\") \n",
    "\n",
    "# Encadeando o componente da esquerda com o da direita \n",
    "chain = prompt_tamplate | llm\n",
    "\n",
    "# Executando o \"chain\" acima\n",
    "response = chain.invoke(input={\"context\":context})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dbd38d",
   "metadata": {},
   "source": [
    "### Workflow\n",
    "\n",
    "*1.Você cria um contexto (um texto grande sobre Cristiano Ronaldo)* \n",
    "\n",
    "* context = \"\"...\"\" \n",
    "\n",
    "* É apenas um texto comum em Python. Ele será enviado ao modelo depois.Pense nele como a informação bruta que você quer que o modelo analise \n",
    "\n",
    "*2.Você cria um prompt com um placeholder {context}* \n",
    "\n",
    "* Esse texto é o template do que você quer perguntar ao modelo. \n",
    "\n",
    "* O {context} dentro do prompt é um “buraco” que será preenchido pela variável context \n",
    "\n",
    "*3.Você cria o PromptTemplate* \n",
    "\n",
    "* Aqui você está dizendo para o LangChain: “O prompt acima tem um espaço chamado context. Sempre que alguém mandar uma variável chamada context, substitua {context} dentro do prompt por esse valor.” Ou seja: O PromptTemplate serve apenas para montar o texto final que irá para o modelo. \n",
    "\n",
    "*4.Você cria o LLM* \n",
    "\n",
    "* Aqui você cria o modelo de IA que vai receber um texto e gerar uma resposta. Ele não faz nada sozinho ainda. Ele só fica esperando um prompt montado. \n",
    "\n",
    "*5.Você cria o Chain* \n",
    "\n",
    "* Isso significa: “Pegue o PromptTemplate, use ele para montar o prompt final, depois envie esse prompt para o modelo.” \n",
    "\n",
    "\n",
    "* É literalmente uma pipeline: (1) PromptTemplate → (2) LLM (Ou seja: o template organiza o texto, o LLM produz a resposta ) \n",
    "\n",
    "*6.response = chain.invoke(input={\"context\":context})* \n",
    "\n",
    "* Aqui ocorre o fluxo completo: 1 - A chain pega a variável context enviada no invoke(); 2 - O PromptTemplate preenche o {context} dentro do prompt; 3 - Esse texto é enviado para o modelo gpt-4.1-mini; 4 - O modelo lê esse prompt pronto e responde; 5 - A resposta final é guardada em response.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM-s-",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
